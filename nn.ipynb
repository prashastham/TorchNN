{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # logits are the unnormalized ouput values of the nn's final layer\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 6\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# nn.Flatten ->  28X28 image to 784 pixel values\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200])\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear -> linear transformation on the input data \n",
    "# (using stored weights and biases)\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=200)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-4.5566e-01, -1.0408e-01,  1.5582e-02, -1.1191e-01, -4.4729e-02,\n",
      "         -5.3997e-01,  3.6226e-01, -5.0750e-01, -4.1135e-01,  4.5544e-02,\n",
      "          1.0232e-01,  3.1147e-01,  1.4578e-01, -5.9325e-01,  2.5307e-01,\n",
      "          6.4199e-02,  7.0377e-01, -6.0155e-02, -3.2041e-02, -7.1822e-02,\n",
      "          6.7404e-02,  1.6131e-01,  5.9427e-01,  4.1027e-01, -2.0111e-01,\n",
      "          2.0341e-02,  2.5911e-01, -2.5476e-01, -3.8917e-01, -6.5688e-01,\n",
      "          2.0638e-01, -1.3138e-01,  3.1536e-01,  4.8270e-01,  4.5204e-01,\n",
      "         -4.4188e-01,  3.0505e-01, -3.5397e-01,  2.9765e-01,  4.5023e-01,\n",
      "         -9.9323e-02,  1.8277e-02, -3.2945e-01,  3.8849e-01,  9.4976e-02,\n",
      "         -6.3139e-02,  2.0086e-01, -5.0283e-01,  2.5494e-01, -1.4506e-01,\n",
      "          3.7121e-01,  1.1820e-01, -2.2860e-03,  1.3887e-01, -1.8320e-01,\n",
      "          1.3583e-01, -1.3554e-01,  1.6044e-01, -8.5494e-02,  5.8181e-02,\n",
      "          1.0796e-01,  2.7751e-01, -7.0210e-02, -2.6001e-01, -3.7628e-02,\n",
      "          2.4802e-01, -3.9362e-01, -4.1830e-01, -1.4233e-01,  2.6226e-02,\n",
      "         -2.2200e-01,  2.4442e-01,  9.0660e-03, -6.5016e-02, -3.7224e-01,\n",
      "          2.1881e-01, -3.3482e-01,  2.2080e-01,  5.5002e-01,  7.8925e-01,\n",
      "          2.5531e-01,  1.2504e-01,  3.3785e-02,  6.2728e-02, -8.0075e-01,\n",
      "          3.0273e-02, -9.1986e-02,  2.1889e-01, -4.2344e-02,  5.1229e-02,\n",
      "         -1.2622e-02,  2.4117e-01,  4.0537e-01, -8.3025e-01,  1.9775e-01,\n",
      "         -1.4038e-01, -4.2634e-01,  4.7872e-01, -2.5951e-01, -1.5848e-01,\n",
      "         -4.1963e-01,  2.5430e-01, -1.8142e-01,  1.2928e-01, -4.0065e-01,\n",
      "         -2.8552e-01, -5.4991e-01, -6.0835e-01,  6.6740e-02,  5.6545e-02,\n",
      "         -3.6831e-01, -2.1418e-01,  9.7284e-02,  4.9337e-01,  8.6923e-02,\n",
      "         -1.4187e-01, -3.9445e-01,  2.5116e-01, -1.6548e-01,  3.0405e-01,\n",
      "          2.8626e-01,  3.1259e-01, -7.8749e-02, -1.8460e-01,  2.1032e-01,\n",
      "         -1.1479e-01, -9.0112e-01, -1.7272e-01, -5.5798e-02,  2.1452e-01,\n",
      "         -3.6506e-01, -1.4865e-01,  3.4026e-01, -1.1934e-01, -3.4177e-01,\n",
      "         -3.2975e-01, -3.3286e-01,  1.4627e-01, -2.6632e-01,  4.8815e-01,\n",
      "          5.0919e-01, -1.1823e-01, -4.2874e-01,  1.0546e-02,  4.6920e-02,\n",
      "         -4.6519e-01,  5.5108e-01, -1.3194e-01, -2.3023e-01, -2.4199e-04,\n",
      "         -2.1933e-01,  3.3049e-01, -5.0793e-01, -5.8031e-02, -7.1820e-01,\n",
      "          4.9864e-01, -5.2936e-02, -3.0941e-01, -2.5104e-02, -2.5870e-01,\n",
      "          3.1291e-01,  4.1339e-01, -6.0132e-01,  4.3538e-01,  1.2796e-01,\n",
      "          2.7200e-01,  3.4843e-01, -1.0259e-01,  1.5335e-01, -1.2637e-03,\n",
      "         -3.3549e-01,  7.9588e-02, -4.0438e-01, -3.8550e-01,  2.3801e-01,\n",
      "         -4.5259e-01,  6.8125e-02, -1.4837e-01,  4.7223e-01,  2.1286e-01,\n",
      "          3.1173e-01,  3.2953e-01,  2.5484e-01, -3.1479e-01, -8.8789e-01,\n",
      "         -2.8796e-01,  6.8328e-02, -1.6158e-01, -5.4113e-01,  1.4081e-01,\n",
      "         -1.8338e-01,  3.9473e-01, -1.4624e-01, -2.9789e-01, -1.3805e-01,\n",
      "         -3.2062e-01,  2.7293e-01,  1.5059e-01, -3.6382e-01,  7.0737e-01],\n",
      "        [-4.4465e-01, -5.8883e-03, -2.6340e-01, -1.4924e-02, -4.1012e-01,\n",
      "         -1.3140e-01, -1.6822e-01, -5.8757e-01, -5.3011e-01,  1.1927e-01,\n",
      "         -4.0164e-02,  4.1567e-02, -4.0238e-02, -4.7685e-01,  3.1027e-01,\n",
      "          1.3946e-02,  6.1630e-01,  1.8223e-01,  8.3463e-02,  2.0061e-02,\n",
      "          8.9036e-02,  1.4350e-01,  4.3036e-01,  2.9271e-01, -1.3737e-01,\n",
      "          1.5107e-01,  3.8683e-01,  5.1466e-02, -2.1415e-01, -5.4641e-01,\n",
      "          1.8313e-01,  1.5803e-01,  2.8916e-01,  6.2254e-01, -6.0115e-03,\n",
      "          3.3108e-03,  2.4104e-01,  1.9886e-01,  2.6710e-01,  3.8380e-01,\n",
      "          3.2220e-01,  4.2083e-01, -5.1985e-01,  3.0821e-01,  1.7136e-01,\n",
      "         -4.1718e-02,  3.0538e-01, -2.4166e-01,  4.9157e-01, -3.0479e-01,\n",
      "          2.2578e-01,  4.5413e-02, -3.9940e-02,  1.9067e-01, -5.2956e-02,\n",
      "          3.8068e-01, -6.6210e-02,  9.1975e-02,  1.8769e-01, -4.5668e-01,\n",
      "         -1.4528e-02,  1.9969e-01,  7.6654e-02,  5.6855e-02, -2.2556e-01,\n",
      "         -1.4406e-01, -6.2628e-01, -8.5361e-02,  1.7262e-01,  4.6526e-01,\n",
      "         -4.3682e-01,  2.1981e-01,  1.9130e-01,  1.9776e-01,  3.9907e-02,\n",
      "          9.6631e-02, -1.3317e-01,  4.2075e-02, -9.0044e-02,  3.5174e-01,\n",
      "          3.0191e-01,  3.5252e-01,  1.0200e-01,  3.0901e-01, -4.1896e-01,\n",
      "          4.7011e-02,  1.3087e-01,  6.7072e-01,  2.1707e-01,  1.6797e-01,\n",
      "          2.3605e-01,  1.0142e-01,  1.3896e-01, -6.6312e-01,  2.7558e-01,\n",
      "         -7.2391e-03, -6.5627e-01,  3.8269e-01,  1.5473e-01, -2.3878e-01,\n",
      "         -3.1398e-01, -1.3382e-01, -4.1102e-01,  1.6362e-01, -2.8492e-01,\n",
      "         -1.0172e-01, -2.3778e-01, -6.3927e-01,  1.6041e-01,  1.5109e-01,\n",
      "         -4.0627e-02,  4.5613e-02,  5.2268e-02,  4.0421e-01,  1.9213e-01,\n",
      "         -7.8915e-02, -2.2586e-01,  5.6513e-01, -1.9041e-01,  5.1807e-01,\n",
      "         -1.0039e-01,  6.5966e-02, -1.3678e-01, -1.2961e-01,  2.9840e-01,\n",
      "         -6.7833e-01, -5.4702e-01, -2.0087e-01, -1.2043e-01,  3.7147e-01,\n",
      "         -4.4207e-01, -1.4026e-01, -1.5060e-01, -2.3863e-01, -5.4188e-01,\n",
      "         -2.1538e-01, -3.0144e-01,  9.4366e-02, -6.6280e-02,  6.3900e-01,\n",
      "          8.6405e-01, -2.4942e-01, -5.7933e-02,  4.0573e-01,  1.5750e-01,\n",
      "         -4.1903e-01,  5.9324e-01,  2.3000e-01, -1.9475e-01,  2.1282e-01,\n",
      "         -1.9548e-01,  1.3268e-01, -5.5595e-01,  9.1359e-02, -8.4701e-01,\n",
      "          3.4048e-01, -3.2170e-02, -1.7348e-01, -2.4718e-01, -6.3642e-02,\n",
      "          3.7961e-01, -7.4607e-02, -4.7879e-02,  6.6462e-01,  6.8993e-02,\n",
      "          6.2090e-01,  1.2514e-01, -1.8556e-01, -3.7314e-02,  2.1539e-01,\n",
      "         -3.6697e-01,  1.2004e-01, -6.0348e-01,  1.4742e-01,  1.3083e-01,\n",
      "         -6.0629e-01, -3.3027e-02, -1.4941e-01,  5.9432e-01,  4.4431e-01,\n",
      "          3.9115e-01,  3.3254e-01,  1.0937e-01, -1.9256e-01, -9.7356e-01,\n",
      "         -8.3169e-02,  8.4573e-02,  1.0052e-01, -6.4221e-01,  3.0213e-01,\n",
      "         -2.3290e-01,  6.4356e-01,  1.9820e-01,  1.3017e-01, -1.0107e-01,\n",
      "         -5.9680e-01,  1.9330e-01, -2.5001e-01, -4.4229e-01,  1.0599e+00],\n",
      "        [-4.9735e-01,  6.9108e-02, -2.4638e-01, -2.4861e-01, -9.2509e-02,\n",
      "          2.9001e-02, -2.1175e-01, -4.4841e-01, -4.4085e-01,  2.7349e-01,\n",
      "          2.8654e-01,  1.7332e-01, -1.2507e-01, -5.2566e-01,  4.7551e-01,\n",
      "         -1.4474e-01,  5.1228e-01,  1.7656e-01, -5.6628e-02,  3.9011e-02,\n",
      "          1.6191e-01,  1.6855e-01,  6.7136e-01,  1.6479e-01, -2.7587e-01,\n",
      "         -6.7567e-02,  3.7156e-01,  2.2418e-02, -1.0666e-01, -1.8019e-01,\n",
      "         -1.7172e-01, -2.4916e-01,  2.3835e-01,  8.0277e-01,  7.6880e-02,\n",
      "         -5.8628e-01,  1.3754e-01, -4.4441e-01,  1.5923e-01,  3.6434e-01,\n",
      "         -7.7561e-02,  2.2909e-01, -6.0850e-01,  1.3491e-02,  2.1781e-01,\n",
      "         -2.5241e-01,  2.5532e-01, -3.8782e-01,  5.6102e-01, -1.0835e-01,\n",
      "          5.7041e-01, -1.7403e-03,  2.7773e-01,  5.2378e-01,  9.6866e-02,\n",
      "          1.8315e-01, -4.2908e-02, -7.7198e-02, -3.6221e-02,  2.7797e-01,\n",
      "         -1.4886e-01,  4.3045e-02, -1.2470e-01, -1.3497e-01, -1.1457e-01,\n",
      "         -3.4971e-01, -2.6987e-01, -2.1182e-01,  2.1573e-01,  5.5095e-02,\n",
      "         -5.9723e-02,  2.9132e-01, -7.0112e-02, -3.4855e-01, -1.1833e-01,\n",
      "          2.0111e-01, -2.3569e-01,  3.7407e-01,  2.6164e-01,  6.5871e-01,\n",
      "          5.4381e-02, -3.4412e-01, -7.8659e-02,  1.1297e-01, -2.1338e-01,\n",
      "         -1.8767e-02, -1.4016e-01,  6.0978e-01, -8.0162e-02,  2.7037e-01,\n",
      "          1.8712e-01,  2.0268e-01,  1.8893e-01, -7.9337e-01, -8.3509e-03,\n",
      "          7.4651e-02, -5.2868e-01,  6.7654e-01, -2.4844e-01, -3.8758e-01,\n",
      "         -2.8874e-01,  8.7110e-02, -2.1609e-01, -2.4303e-02, -1.3922e-01,\n",
      "         -2.7876e-01, -5.5315e-01, -6.9709e-01, -2.3987e-01,  8.0117e-02,\n",
      "         -2.3767e-01, -4.8168e-03, -5.9104e-01,  4.7389e-01,  1.0668e-03,\n",
      "         -1.5217e-01, -2.1901e-01,  5.6897e-01, -4.4707e-01,  4.0426e-01,\n",
      "          3.6760e-01,  9.6747e-02, -5.6083e-02, -4.9427e-01,  3.1931e-01,\n",
      "         -3.8587e-01, -5.5561e-01,  2.3806e-01, -3.7362e-01,  3.9844e-02,\n",
      "         -4.2244e-01, -8.7487e-02,  7.8948e-02,  5.6166e-02, -4.2491e-01,\n",
      "          4.6880e-02, -3.3951e-01, -8.2140e-02, -2.3723e-01,  2.1476e-01,\n",
      "          6.2291e-01, -2.8338e-01, -2.1768e-01,  1.3367e-01, -1.1730e-01,\n",
      "         -4.7932e-01,  4.7629e-01, -1.7465e-01,  1.1038e-01,  2.5056e-01,\n",
      "         -1.0161e-01,  6.9089e-02, -2.8784e-01, -3.0416e-02, -4.3959e-01,\n",
      "          3.3162e-01, -1.8732e-01, -4.4150e-01, -1.2025e-01, -3.7375e-01,\n",
      "          4.1196e-01, -4.6134e-02, -2.1984e-01,  4.1062e-02,  5.3562e-02,\n",
      "          4.9041e-01,  3.0593e-01, -1.9081e-01,  1.5487e-01,  1.6418e-01,\n",
      "         -4.2308e-01, -1.6313e-01, -8.0961e-01,  6.5071e-02,  2.2238e-01,\n",
      "         -5.6289e-01, -1.1857e-02, -2.9936e-01,  3.0616e-01, -4.1349e-02,\n",
      "          6.1338e-01,  3.7407e-01, -1.3960e-01, -1.4329e-01, -9.6507e-01,\n",
      "          3.5724e-02,  3.3831e-01,  9.6982e-02, -3.9806e-01,  2.2458e-02,\n",
      "          6.1199e-02,  1.3482e-01,  3.5916e-01, -2.9041e-01, -2.0421e-01,\n",
      "         -5.8862e-02,  2.0646e-01, -2.3214e-01, -5.1053e-01,  7.2054e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.0156, 0.0000, 0.0000, 0.0000, 0.3623, 0.0000, 0.0000,\n",
      "         0.0455, 0.1023, 0.3115, 0.1458, 0.0000, 0.2531, 0.0642, 0.7038, 0.0000,\n",
      "         0.0000, 0.0000, 0.0674, 0.1613, 0.5943, 0.4103, 0.0000, 0.0203, 0.2591,\n",
      "         0.0000, 0.0000, 0.0000, 0.2064, 0.0000, 0.3154, 0.4827, 0.4520, 0.0000,\n",
      "         0.3051, 0.0000, 0.2976, 0.4502, 0.0000, 0.0183, 0.0000, 0.3885, 0.0950,\n",
      "         0.0000, 0.2009, 0.0000, 0.2549, 0.0000, 0.3712, 0.1182, 0.0000, 0.1389,\n",
      "         0.0000, 0.1358, 0.0000, 0.1604, 0.0000, 0.0582, 0.1080, 0.2775, 0.0000,\n",
      "         0.0000, 0.0000, 0.2480, 0.0000, 0.0000, 0.0000, 0.0262, 0.0000, 0.2444,\n",
      "         0.0091, 0.0000, 0.0000, 0.2188, 0.0000, 0.2208, 0.5500, 0.7893, 0.2553,\n",
      "         0.1250, 0.0338, 0.0627, 0.0000, 0.0303, 0.0000, 0.2189, 0.0000, 0.0512,\n",
      "         0.0000, 0.2412, 0.4054, 0.0000, 0.1978, 0.0000, 0.0000, 0.4787, 0.0000,\n",
      "         0.0000, 0.0000, 0.2543, 0.0000, 0.1293, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0667, 0.0565, 0.0000, 0.0000, 0.0973, 0.4934, 0.0869, 0.0000, 0.0000,\n",
      "         0.2512, 0.0000, 0.3041, 0.2863, 0.3126, 0.0000, 0.0000, 0.2103, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2145, 0.0000, 0.0000, 0.3403, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1463, 0.0000, 0.4881, 0.5092, 0.0000, 0.0000, 0.0105,\n",
      "         0.0469, 0.0000, 0.5511, 0.0000, 0.0000, 0.0000, 0.0000, 0.3305, 0.0000,\n",
      "         0.0000, 0.0000, 0.4986, 0.0000, 0.0000, 0.0000, 0.0000, 0.3129, 0.4134,\n",
      "         0.0000, 0.4354, 0.1280, 0.2720, 0.3484, 0.0000, 0.1534, 0.0000, 0.0000,\n",
      "         0.0796, 0.0000, 0.0000, 0.2380, 0.0000, 0.0681, 0.0000, 0.4722, 0.2129,\n",
      "         0.3117, 0.3295, 0.2548, 0.0000, 0.0000, 0.0000, 0.0683, 0.0000, 0.0000,\n",
      "         0.1408, 0.0000, 0.3947, 0.0000, 0.0000, 0.0000, 0.0000, 0.2729, 0.1506,\n",
      "         0.0000, 0.7074],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1193, 0.0000, 0.0416, 0.0000, 0.0000, 0.3103, 0.0139, 0.6163, 0.1822,\n",
      "         0.0835, 0.0201, 0.0890, 0.1435, 0.4304, 0.2927, 0.0000, 0.1511, 0.3868,\n",
      "         0.0515, 0.0000, 0.0000, 0.1831, 0.1580, 0.2892, 0.6225, 0.0000, 0.0033,\n",
      "         0.2410, 0.1989, 0.2671, 0.3838, 0.3222, 0.4208, 0.0000, 0.3082, 0.1714,\n",
      "         0.0000, 0.3054, 0.0000, 0.4916, 0.0000, 0.2258, 0.0454, 0.0000, 0.1907,\n",
      "         0.0000, 0.3807, 0.0000, 0.0920, 0.1877, 0.0000, 0.0000, 0.1997, 0.0767,\n",
      "         0.0569, 0.0000, 0.0000, 0.0000, 0.0000, 0.1726, 0.4653, 0.0000, 0.2198,\n",
      "         0.1913, 0.1978, 0.0399, 0.0966, 0.0000, 0.0421, 0.0000, 0.3517, 0.3019,\n",
      "         0.3525, 0.1020, 0.3090, 0.0000, 0.0470, 0.1309, 0.6707, 0.2171, 0.1680,\n",
      "         0.2360, 0.1014, 0.1390, 0.0000, 0.2756, 0.0000, 0.0000, 0.3827, 0.1547,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1636, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1604, 0.1511, 0.0000, 0.0456, 0.0523, 0.4042, 0.1921, 0.0000, 0.0000,\n",
      "         0.5651, 0.0000, 0.5181, 0.0000, 0.0660, 0.0000, 0.0000, 0.2984, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3715, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0944, 0.0000, 0.6390, 0.8641, 0.0000, 0.0000, 0.4057,\n",
      "         0.1575, 0.0000, 0.5932, 0.2300, 0.0000, 0.2128, 0.0000, 0.1327, 0.0000,\n",
      "         0.0914, 0.0000, 0.3405, 0.0000, 0.0000, 0.0000, 0.0000, 0.3796, 0.0000,\n",
      "         0.0000, 0.6646, 0.0690, 0.6209, 0.1251, 0.0000, 0.0000, 0.2154, 0.0000,\n",
      "         0.1200, 0.0000, 0.1474, 0.1308, 0.0000, 0.0000, 0.0000, 0.5943, 0.4443,\n",
      "         0.3911, 0.3325, 0.1094, 0.0000, 0.0000, 0.0000, 0.0846, 0.1005, 0.0000,\n",
      "         0.3021, 0.0000, 0.6436, 0.1982, 0.1302, 0.0000, 0.0000, 0.1933, 0.0000,\n",
      "         0.0000, 1.0599],\n",
      "        [0.0000, 0.0691, 0.0000, 0.0000, 0.0000, 0.0290, 0.0000, 0.0000, 0.0000,\n",
      "         0.2735, 0.2865, 0.1733, 0.0000, 0.0000, 0.4755, 0.0000, 0.5123, 0.1766,\n",
      "         0.0000, 0.0390, 0.1619, 0.1686, 0.6714, 0.1648, 0.0000, 0.0000, 0.3716,\n",
      "         0.0224, 0.0000, 0.0000, 0.0000, 0.0000, 0.2383, 0.8028, 0.0769, 0.0000,\n",
      "         0.1375, 0.0000, 0.1592, 0.3643, 0.0000, 0.2291, 0.0000, 0.0135, 0.2178,\n",
      "         0.0000, 0.2553, 0.0000, 0.5610, 0.0000, 0.5704, 0.0000, 0.2777, 0.5238,\n",
      "         0.0969, 0.1831, 0.0000, 0.0000, 0.0000, 0.2780, 0.0000, 0.0430, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.0551, 0.0000, 0.2913,\n",
      "         0.0000, 0.0000, 0.0000, 0.2011, 0.0000, 0.3741, 0.2616, 0.6587, 0.0544,\n",
      "         0.0000, 0.0000, 0.1130, 0.0000, 0.0000, 0.0000, 0.6098, 0.0000, 0.2704,\n",
      "         0.1871, 0.2027, 0.1889, 0.0000, 0.0000, 0.0747, 0.0000, 0.6765, 0.0000,\n",
      "         0.0000, 0.0000, 0.0871, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0801, 0.0000, 0.0000, 0.0000, 0.4739, 0.0011, 0.0000, 0.0000,\n",
      "         0.5690, 0.0000, 0.4043, 0.3676, 0.0967, 0.0000, 0.0000, 0.3193, 0.0000,\n",
      "         0.0000, 0.2381, 0.0000, 0.0398, 0.0000, 0.0000, 0.0789, 0.0562, 0.0000,\n",
      "         0.0469, 0.0000, 0.0000, 0.0000, 0.2148, 0.6229, 0.0000, 0.0000, 0.1337,\n",
      "         0.0000, 0.0000, 0.4763, 0.0000, 0.1104, 0.2506, 0.0000, 0.0691, 0.0000,\n",
      "         0.0000, 0.0000, 0.3316, 0.0000, 0.0000, 0.0000, 0.0000, 0.4120, 0.0000,\n",
      "         0.0000, 0.0411, 0.0536, 0.4904, 0.3059, 0.0000, 0.1549, 0.1642, 0.0000,\n",
      "         0.0000, 0.0000, 0.0651, 0.2224, 0.0000, 0.0000, 0.0000, 0.3062, 0.0000,\n",
      "         0.6134, 0.3741, 0.0000, 0.0000, 0.0000, 0.0357, 0.3383, 0.0970, 0.0000,\n",
      "         0.0225, 0.0612, 0.1348, 0.3592, 0.0000, 0.0000, 0.0000, 0.2065, 0.0000,\n",
      "         0.0000, 0.7205]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.ReLU -> non linear activation function\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nn.Sequential -> ordered container of modules\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "print(seq_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_probab.shape:\n",
      " tensor([[0.0923, 0.0962, 0.0942, 0.0859, 0.1076, 0.1117, 0.0965, 0.1038, 0.1033,\n",
      "         0.1085],\n",
      "        [0.0920, 0.0962, 0.0824, 0.0885, 0.1101, 0.1032, 0.0887, 0.0961, 0.1230,\n",
      "         0.1197],\n",
      "        [0.1048, 0.0892, 0.0836, 0.0916, 0.1111, 0.1072, 0.0942, 0.0936, 0.1087,\n",
      "         0.1160]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## nn.Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(\"pred_probab.shape:\\n\", pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: \n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0208, -0.0183,  0.0098,  ..., -0.0051,  0.0027, -0.0294],\n",
      "        [ 0.0014, -0.0090,  0.0018,  ...,  0.0155, -0.0238,  0.0081]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0284,  0.0080], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0113, -0.0406,  0.0092,  ..., -0.0183,  0.0337,  0.0027],\n",
      "        [ 0.0384,  0.0147, -0.0105,  ..., -0.0151, -0.0119,  0.0204]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0016, -0.0064], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0389, -0.0424,  0.0194,  ..., -0.0289, -0.0300, -0.0245],\n",
      "        [-0.0407,  0.0404,  0.0208,  ...,  0.0350, -0.0356,  0.0332]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0220, -0.0115], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: \\n{model} \\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
